{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wPCdU6gXv9KZ"
   },
   "source": [
    "# Tumor classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Classification code is done by Chitipotu Kushwanth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "M-bjNYnDqtWh"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import nibabel as nib\n",
    "\n",
    "# Function to classify tumor grade based on tumor subregion volumes\n",
    "def classify_tumor_grade(segmented_image):\n",
    "    \"\"\"\n",
    "    Classifies a tumor as high-grade (1) or low-grade (0) based on the volume of tumor subregions.\n",
    "    \"\"\"\n",
    "    # Tumor subregion labels\n",
    "    LABEL_NECROTIC = 1\n",
    "    LABEL_EDEMA = 2\n",
    "\n",
    "    # Calculate volumes (number of voxels) for each subregion\n",
    "    necrotic_volume = torch.sum(segmented_image == LABEL_NECROTIC).item()\n",
    "    edema_volume = torch.sum(segmented_image == LABEL_EDEMA).item()\n",
    "\n",
    "    # Total tumor volume (sum of all subregions)\n",
    "    total_tumor_volume = necrotic_volume + edema_volume\n",
    "\n",
    "    # Avoid division by zero in case there is no tumor\n",
    "    if total_tumor_volume == 0:\n",
    "        return 0  # Default to low-grade if no tumor is present\n",
    "\n",
    "    # Calculate proportions of necrotic regions relative to total tumor volume\n",
    "    necrotic_proportion = necrotic_volume / total_tumor_volume\n",
    "\n",
    "    # Define thresholds for classification\n",
    "    NECROTIC_THRESHOLD = 0.2  # High-grade tumors tend to have more necrosis\n",
    "\n",
    "    # Classification rule\n",
    "    if necrotic_proportion > NECROTIC_THRESHOLD:\n",
    "        return 1  # High-grade\n",
    "    else:\n",
    "        return 0  # Low-grade\n",
    "\n",
    "# Function to process and classify tumor grade, then save the result in the same folder\n",
    "def process_and_classify(patient_folder):\n",
    "    \"\"\"\n",
    "    Classifies tumor grade based on the segmentation file in the patient's folder and saves the result.\n",
    "    \"\"\"\n",
    "    patient_id = os.path.basename(patient_folder)\n",
    "    seg_file = os.path.join(patient_folder, f\"{patient_id}_seg.nii\")\n",
    "\n",
    "    if os.path.exists(seg_file):\n",
    "        # Load segmentation file\n",
    "        seg_img = nib.load(seg_file)\n",
    "        seg_data = seg_img.get_fdata()\n",
    "\n",
    "        # Convert segmentation data to a PyTorch tensor\n",
    "        segmented_image = torch.tensor(seg_data, dtype=torch.float32)\n",
    "\n",
    "        # Classify tumor grade\n",
    "        grade = classify_tumor_grade(segmented_image)\n",
    "        grade_str = \"High-grade\" if grade == 1 else \"Low-grade\"\n",
    "\n",
    "        # Save classification result in the same folder\n",
    "        classification_file = os.path.join(patient_folder, \"classification.txt\")\n",
    "        with open(classification_file, \"w\") as f:\n",
    "            f.write(f\"Patient ID: {patient_id}\\n\")\n",
    "            f.write(f\"Tumor grade: {grade_str}\\n\")\n",
    "\n",
    "        print(f\"Classification saved for patient {patient_id}: {grade_str}\")\n",
    "    else:\n",
    "        print(f\"Segmentation file not found for patient {patient_id}\")\n",
    "\n",
    "# Mount Google Drive if needed\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# Main directory containing patient folders\n",
    "input_dir = \"/content/drive/MyDrive/Processed_BraTS2021/val\"\n",
    "# Process each patient folder\n",
    "for patient_folder in os.listdir(input_dir):\n",
    "    full_path = os.path.join(input_dir, patient_folder)\n",
    "    if os.path.isdir(full_path):  # Check if it's a folder\n",
    "        process_and_classify(full_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "onFPdJefwLhp"
   },
   "source": [
    "# EfficientNEt3D"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rest of the code is from   Sai satwik clasifier code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZXNzKuvfsmHP"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class EfficientNet3D(nn.Module):\n",
    "    def __init__(self, input_channels=1, num_classes=2):  # Default input channels to 1\n",
    "        super(EfficientNet3D, self).__init__()\n",
    "\n",
    "        # Initial Conv3D layer with dynamic input channels (e.g., 1 for grayscale or 3 for multi-channel)\n",
    "        self.conv1 = nn.Conv3d(input_channels, 32, kernel_size=3, stride=1, padding=1)\n",
    "        self.bn1 = nn.BatchNorm3d(32)\n",
    "        self.relu1 = nn.ReLU()\n",
    "\n",
    "        # Efficient 3D blocks (a few layers for simplicity)\n",
    "        self.block1 = self._make_block(32, 64)\n",
    "        self.block2 = self._make_block(64, 128)\n",
    "        self.block3 = self._make_block(128, 256)\n",
    "\n",
    "        # Global average pooling layer (to handle variable input size)\n",
    "        self.global_avg_pool = nn.AdaptiveAvgPool3d(1)\n",
    "\n",
    "        # Fully connected layers for classification\n",
    "        self.fc1 = nn.Linear(256, 512)\n",
    "        self.fc2 = nn.Linear(512, num_classes)  # Output layer changed to 2 classes\n",
    "\n",
    "        # Dropout for regularization\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "\n",
    "    def _make_block(self, in_channels, out_channels):\n",
    "        # A helper function to create 3D convolution blocks\n",
    "        return nn.Sequential(\n",
    "            nn.Conv3d(in_channels, out_channels, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm3d(out_channels),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool3d(2)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Pass through the initial convolution and the blocks\n",
    "        x = self.relu1(self.bn1(self.conv1(x)))\n",
    "\n",
    "        # Block 1, Block 2, Block 3\n",
    "        x = self.block1(x)\n",
    "        x = self.block2(x)\n",
    "        x = self.block3(x)\n",
    "\n",
    "        # Global average pooling to ensure size consistency\n",
    "        x = self.global_avg_pool(x)\n",
    "        x = torch.flatten(x, 1)  # Flatten to (batch_size, num_features)\n",
    "\n",
    "        # Fully connected layers\n",
    "        x = self.dropout(F.relu(self.fc1(x)))\n",
    "        x = self.fc2(x)  # No sigmoid here because we'll use softmax during loss computation\n",
    "\n",
    "        return x\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UzmGGC-uwP8Y"
   },
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rest is by Chitipotu Kushwanth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Fb_UoEWRxEdi"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import nibabel as nib\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision import transforms\n",
    "\n",
    "class BRATS2021Dataset(Dataset):\n",
    "    def __init__(self, root_dir, transform=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            root_dir (string): Root directory where each patient folder contains segmentation (.nii) and classification.txt.\n",
    "            transform (callable, optional): Optional transform to be applied on a sample.\n",
    "        \"\"\"\n",
    "        self.root_dir = root_dir\n",
    "        self.patients = sorted(os.listdir(root_dir))  # List of patient folders\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.patients)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        patient_folder = self.patients[idx]\n",
    "        patient_dir = os.path.join(self.root_dir, patient_folder)\n",
    "\n",
    "        # Extract patient ID (assuming folder name format is like \"BraTS2021_00005\")\n",
    "        patient_id = patient_folder\n",
    "\n",
    "        # Define the segmentation file path\n",
    "        segmentation_path = os.path.join(patient_dir, f\"{patient_id}_seg.nii\")\n",
    "        if not os.path.exists(segmentation_path):\n",
    "            raise FileNotFoundError(f\"Segmentation file not found: {segmentation_path}\")\n",
    "\n",
    "        # Load the NIfTI file using nibabel\n",
    "        nii = nib.load(segmentation_path)\n",
    "        segmentation = nii.get_fdata()  # Get voxel data (numpy array)\n",
    "\n",
    "        # Convert segmentation to a torch tensor\n",
    "        segmentation = torch.tensor(segmentation, dtype=torch.float32)\n",
    "\n",
    "        # Parse the classification.txt file to get the tumor grade\n",
    "        classification_file = os.path.join(patient_dir, \"classification.txt\")\n",
    "        if not os.path.exists(classification_file):\n",
    "            raise FileNotFoundError(f\"Classification file not found: {classification_file}\")\n",
    "\n",
    "        with open(classification_file, 'r') as f:\n",
    "            lines = f.readlines()\n",
    "            tumor_grade = lines[1].strip()  # Assuming second line contains the tumor grade\n",
    "\n",
    "        # Remove the prefix \"Tumor grade: \" if present\n",
    "        tumor_grade = tumor_grade.replace(\"Tumor grade: \", \"\").strip()\n",
    "\n",
    "        # Convert tumor grade to a numeric label (e.g., 0 for low-grade, 1 for high-grade)\n",
    "        if tumor_grade == \"Low-grade\":\n",
    "            tumor_label = 0\n",
    "        elif tumor_grade == \"High-grade\":\n",
    "            tumor_label = 1\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown tumor grade: {tumor_grade}\")\n",
    "\n",
    "        # Apply transformations if any\n",
    "        if self.transform:\n",
    "            segmentation = self.transform(segmentation)\n",
    "\n",
    "        # Return the segmentation and corresponding tumor label\n",
    "        return segmentation, tumor_label\n",
    "\n",
    "\n",
    "# Example transformation (e.g., normalization, resizing)\n",
    "transform = transforms.Compose([\n",
    "    # Add any transformations needed, for example:\n",
    "    transforms.Normalize(mean=[0.5], std=[0.5]),  # Example: Normalize the data\n",
    "])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SyQkPBy21Wk-"
   },
   "outputs": [],
   "source": [
    "# Example usage\n",
    "train_dir = r\"C:\\Users\\intel5\\Downloads\\Processed_BraTS2021-20241121T100710Z-001\\Processed_BraTS2021\\train\"   # Replace with the actual path\n",
    "dataset = BRATS2021Dataset(root_dir=train_dir, transform=transform)\n",
    "\n",
    "# Example DataLoader\n",
    "train_loader = torch.utils.data.DataLoader(dataset, batch_size=2, shuffle=True)\n",
    "\n",
    "val_dir = r\"C:\\Users\\intel5\\Downloads\\Processed_BraTS2021-20241121T100710Z-001\\Processed_BraTS2021\\val\"   # Replace with the actual path\n",
    "dataset = BRATS2021Dataset(root_dir=val_dir, transform=transform)\n",
    "\n",
    "# Example DataLoader\n",
    "val_loader = torch.utils.data.DataLoader(dataset, batch_size=2, shuffle=True)\n",
    "\n",
    "test_dir = r\"C:\\Users\\intel5\\Downloads\\Processed_BraTS2021-20241121T100710Z-001\\Processed_BraTS2021\\test\"   # Replace with the actual path\n",
    "dataset = BRATS2021Dataset(root_dir=test_dir, transform=transform)\n",
    "\n",
    "# Example DataLoader\n",
    "test_loader = torch.utils.data.DataLoader(dataset, batch_size=2, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TUgx2YZ0wSg3"
   },
   "source": [
    "# Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "id": "iXUBE-Ka3HIE",
    "outputId": "7e5f3059-bddb-4e17-ad99-32cd1c80df56"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\intel5\\AppData\\Local\\Temp\\ipykernel_13036\\3377354111.py:46: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(checkpoint_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading checkpoint from C:\\Users\\intel5\\Downloads\\Processed_BraTS2021-20241121T100710Z-001\\Processed_BraTS2021\\checkpoints\\checkpoint_epoch_1.pth\n",
      "Checkpoint loaded. Starting from epoch 1. Last loss: 0.6794873289253613\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/10: 100%|█████████████████████████████████████████████████████████████████| 438/438 [52:19<00:00,  7.17s/batch]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/10, Loss: 0.6634, Accuracy: 62.29%\n",
      "Checkpoint saved at epoch 2, filename: checkpoint_epoch_2.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/10: 100%|█████████████████████████████████████████████████████████████████| 438/438 [51:42<00:00,  7.08s/batch]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/10, Loss: 0.6675, Accuracy: 63.20%\n",
      "Checkpoint saved at epoch 3, filename: checkpoint_epoch_3.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/10:   2%|█                                                                  | 7/438 [00:50<52:04,  7.25s/batch]"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.metrics import accuracy_score\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "\n",
    "# Assuming your dataset is set up with DataLoader as `train_loader` and `test_loader`\n",
    "\n",
    "# Initialize the model\n",
    "model = EfficientNet3D(input_channels=1, num_classes=2)  # Number of classes is 2\n",
    "\n",
    "# Check if GPU is available and move the model to GPU if possible\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)\n",
    "\n",
    "# Define the loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()  # For classification tasks\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Directory to save checkpoints\n",
    "checkpoint_dir = r'C:\\Users\\intel5\\Downloads\\Processed_BraTS2021-20241121T100710Z-001\\Processed_BraTS2021\\checkpoints'\n",
    "\n",
    "# Function to save checkpoints\n",
    "def save_checkpoint(model, optimizer, epoch, loss, checkpoint_dir):\n",
    "    filename = f\"checkpoint_epoch_{epoch+1}.pth\"  # Save checkpoint with epoch number\n",
    "    checkpoint = {\n",
    "        'epoch': epoch,\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        'loss': loss,\n",
    "    }\n",
    "\n",
    "    # Ensure the checkpoint directory exists\n",
    "    os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "\n",
    "    checkpoint_path = os.path.join(checkpoint_dir, filename)\n",
    "    torch.save(checkpoint, checkpoint_path)\n",
    "    print(f\"Checkpoint saved at epoch {epoch+1}, filename: {filename}\")\n",
    "\n",
    "# Function to load checkpoints\n",
    "def load_checkpoint(model, optimizer, checkpoint_dir, filename=\"checkpoint_epoch_1.pth\"):\n",
    "    checkpoint_path = os.path.join(checkpoint_dir, filename)\n",
    "    if os.path.isfile(checkpoint_path):\n",
    "        print(f\"Loading checkpoint from {checkpoint_path}\")\n",
    "        checkpoint = torch.load(checkpoint_path)\n",
    "        model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "        start_epoch = checkpoint['epoch'] + 1  # Resume from the next epoch\n",
    "        loss = checkpoint['loss']\n",
    "        print(f\"Checkpoint loaded. Starting from epoch {start_epoch}. Last loss: {loss}\")\n",
    "        return model, optimizer, start_epoch, loss\n",
    "    else:\n",
    "        print(\"No checkpoint found, starting from scratch.\")\n",
    "        return model, optimizer, 0, None\n",
    "\n",
    "# Training function with tqdm and checkpoint saving\n",
    "def train_model(model, train_loader, criterion, optimizer, device, epochs=10, checkpoint_dir=None):\n",
    "    model.train()  # Set the model to training mode\n",
    "    start_epoch = 0  # Default start epoch\n",
    "    if checkpoint_dir:\n",
    "        model, optimizer, start_epoch, _ = load_checkpoint(model, optimizer, checkpoint_dir)\n",
    "\n",
    "    for epoch in range(start_epoch, epochs):\n",
    "        running_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "\n",
    "        # Wrap the DataLoader with tqdm to show progress\n",
    "        for inputs, labels in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{epochs}\", unit=\"batch\"):\n",
    "            inputs = inputs.to(device)  # Move to GPU if available\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            # Zero the gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = model(inputs)\n",
    "\n",
    "            # Compute loss\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            # Backward pass and optimize\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # Track loss and accuracy\n",
    "            running_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "\n",
    "        epoch_loss = running_loss / len(train_loader)\n",
    "        epoch_accuracy = 100 * correct / total\n",
    "\n",
    "        print(f\"Epoch {epoch+1}/{epochs}, Loss: {epoch_loss:.4f}, Accuracy: {epoch_accuracy:.2f}%\")\n",
    "\n",
    "        # Save checkpoint after every epoch with the epoch number in the filename\n",
    "        if checkpoint_dir:\n",
    "            save_checkpoint(model, optimizer, epoch, epoch_loss, checkpoint_dir)\n",
    "\n",
    "# Evaluation function with tqdm\n",
    "def evaluate_model(model, test_loader, device):\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "\n",
    "    with torch.no_grad():  # Disable gradient computation for evaluation\n",
    "        # Wrap the DataLoader with tqdm to show progress\n",
    "        for inputs, labels in tqdm(test_loader, desc=\"Evaluating\", unit=\"batch\"):\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = model(inputs)\n",
    "\n",
    "            # Get predictions\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            all_preds.extend(predicted.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "    # Calculate accuracy\n",
    "    accuracy = accuracy_score(all_labels, all_preds)\n",
    "    print(f\"Test Accuracy: {accuracy * 100:.2f}%\")\n",
    "    return accuracy\n",
    "\n",
    "# Train the model\n",
    "train_model(model, train_loader, criterion, optimizer, device, epochs=10, checkpoint_dir=checkpoint_dir)\n",
    "\n",
    "# Evaluate the model\n",
    "evaluate_model(model, val_loader, device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pIA8gcXcvuOU"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
